#!/usr/bin/env python3

# A sample training component that trains a naive bayes model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.

from __future__ import print_function

import os
import json
import pickle
import sys
import traceback
import subprocess

import pandas as pd

from config_and_helper import *

# These are the paths to where SageMaker mounts interesting things in your container.
prefix = '/opt/ml/'
input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_name='training'
training_path = os.path.join(input_path, channel_name)
input_csv_file = os.listdir(training_path)[0]

# The function to execute the training.
def train():
    print('Starting the training.')
    try:
        print("Preprocessing data...")
        raw_training_data = pd.read_csv(os.path.join(training_path, input_csv_file), ';')
        data_output, label_output, _, _, _ = training_data_preprocessing(raw_training_data)
        data_output.to_csv('cache/preprocessed_training_data.csv', index=False)
        label_output.to_csv('cache/preprocessed_training_labels.csv', index=False)
        training_data = pd.read_csv('cache/preprocessed_training_data.csv')
        training_labels = pd.read_csv('cache/preprocessed_training_labels.csv')

        print("Start to train...")
        # new f1 parameters
        params = {'bagging_fraction': 0.9500000000000001, 'boosting': 'gbdt', 'feature_fraction': 0.6900000000000001, 
                  'is_unbalance': True, 'lambda_l1': 0.46353553434088546, 'lambda_l2': 3.3532184860573264, 
                  'learning_rate': 0.07116686982192068, 'max_bin': 103, 'max_depth': 13, 'min_data_in_bin': 230, 
                  'min_data_in_leaf': 1, 'min_gain_to_split': 0.15, 'num_leaves': 139, 'objective': 'binary', 
                  'subsample': 0.6845480549505643}
        training_data = training_data.sample(frac=1, random_state=25).reset_index(drop=True)
        training_labels = training_labels.sample(frac=1, random_state=25).reset_index(drop=True)
        test_data = training_data.sample(frac=1/5, random_state=32, replace=False)
        test_labels = training_labels.sample(frac=1/5, random_state=32, replace=False)
        train_model(training_data,training_labels, test_data, test_labels,
                    params, model_path_name=os.path.join(model_path, 'lgb'))

        # store cache files to model path so they can be used lately
        copy_files('cache', model_path, ['extended_columns.pkl', 'num_date_columns.pkl'])

        print('Training complete.')
        
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
